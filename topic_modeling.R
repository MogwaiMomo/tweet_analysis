# working through the following tutorial on LDA topic modeling
# https://www.tidytextmining.com/topicmodeling.html

library(topicmodels)
library(tm)
library(hunspell)

# Step 1: create a TDM object from my collection of tweets using the TM package (try going from tidy objects to TDMs and back)

# Step 1a: create a tidy object from the 'texts' df

# Sidebar: Use cleaning/preprocessing steps from this tweet-specific vignette: https://vsokolov.org/courses/41000/notes/trump-tweets.html

# Create "vectorised stemming/spellcheck function" using hunspell: 
my_hunspell_stem <- function(word) {
  stem_word <- hunspell_stem(word)[[1]]
  if (length(stem_word) == 0) return(word) else return(stem_word[1])
}
vec_hunspell_stem <- Vectorize(my_hunspell_stem, "word")

# Clean/preprocess tweets 
texts %>%
  mutate(text = str_replace_all(text, 
                                pattern=regex("(www|https?[^\\s]+)"),
                                replacement = "")) %>% #rm urls
  mutate(text = str_replace_all(text,
                                pattern = "[[:digit:]]",
                                replacement = "")) %>%
  unnest_tokens(word, text) %>%
  mutate(word = vec_hunspell_stem(word)) %>%
  anti_join(stop_words) %>% 
  count(element_id, word, sort = TRUE) %>%
  rename(document = element_id, term = word, count = n)  -> texts_td
  
# Step 1b: create TDM using the cast function

texts_td %>%
  cast_dtm(document, term, count) -> texts_dtm

# Step 2: geneerate LDA model from dtm

texts_lda <- LDA(texts_dtm, k = 3, control = list(seed = 1234))

# Step 2a: tidy it up so you can see a table of probabilities per-word of being generated by each topic in your model

texts_topics <- tidy(texts_lda, matrix = "beta")

# Sidebar analysis: what are the top 10 most frequent terms per topic? 

texts_top_terms_per_topic <- texts_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

# graph it & export
texts_top_terms_per_topic %>%
  mutate(term = reorder_within(term, beta, topic)) %>% 
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = F) +
  facet_wrap(~ topic, scales = "free" ) + 
  scale_y_reordered() # removes the ____ and topic number from y-axis

ggsave("output/top_terms_per_topic.png")

save_as_csv(texts_td, file_name="data/tidy_tweets.csv")


