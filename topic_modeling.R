# working through the following tutorial on LDA topic modeling
# https://www.tidytextmining.com/topicmodeling.html

library("topicmodels")
library("tm")

# Step 1: create a TDM object from my collection of tweets using the TM package (try going from tidy objects to TDMs and back)

# Step 1a: create a tidy object from the 'texts' df

# Need a custom tibble of twitter-related noise terms:


texts %>%
  select(-hashtags) %>% # remove hashtags variable, which we're not using
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>% 
  count(element_id, word, sort = TRUE) %>%
  rename(document = element_id, term = word, count = n)  -> texts_td
  
# Step 1b: create TDM using the cast function

texts_td %>%
  cast_dtm(document, term, count) -> texts_dtm

# Step 2: geneerate LDA model from dtm

texts_lda <- LDA(texts_dtm, k = 3, control = list(seed = 1234))

# Step 2a: tidy it up so you can see a table of probabilities per-word of being generated by each topic in your model

texts_topics <- tidy(texts_lda, matrix = "beta")

# Sidebar analysis: what are the top 10 most frequent terms per topic? 

texts_top_terms_per_topic <- texts_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

# graph it & export
texts_top_terms_per_topic %>%
  mutate(term = reorder_within(term, beta, topic)) %>% 
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = F) +
  facet_wrap(~ topic, scales = "free" ) + 
  scale_y_reordered() # removes the ____ and topic number from y-axis

ggsave("output/top_terms_per_topic.png")

# NOTE: these results suck because of the URLs and stopwords. Need to remove them

# Removed stopwords with anti_join(). Next: make a custom table of twitter noise terms (t.co, https://, hashtags, etc.)