---
title: "Tweet Collector"
author: "Momoko Price"
date: "10/15/2020"
output: 
  html_document:
    css: style.css
---

***
```{r default_setup, include=FALSE}
wd <- dirname(rstudioapi::getActiveDocumentContext()$path)
setwd(wd)

knitr::opts_chunk$set(include = T,
                      echo = F, 
                      message = F, 
                      warning = F,
                      fig.align = 'left',
                      out.width = '80%')


# global options
options(stringsAsFactors = F)
# Load libraries
require(tidyverse)
require(data.table)
require(ggplot2)
require(gridExtra)
require(PerformanceAnalytics)
require(magrittr)
# load twitter library - the rtweet library is recommended now over twitteR
library(rtweet)
# tools for cleaning data fast
library(janitor)
# for flattening lists
library(purrr)

```
##### Step 1: Authenticate the app

Grab app name, API key and API secret from the credentials saved in 1Password:
 
```{r authenticate, echo=T}
# whatever name you assigned to your created app
appname <- "voc-collector"

# api key (example below is not a real key)
key <- "xsBRv4HDRFw3YRe1jg3lTWxz8"

# api secret (example below is not a real key)
secret <- "1Qf0flgv8R8Qia96SNFtJ7JjEoVsHAoNJ4gD0tDcoZDHh3ZpPi"

# access_token
access_token <- "3302721691-53elyQLaCZwWpAiCImnxBfm48mqmeLt6QqjDz2f"

# access_secret
access_secret <- "W4H4xpBLw3g6VkYrUtJEuIYaM2JQuSZZ58FKWm2m3Nq7U"

# create token named "twitter_token"
twitter_token <- create_token(
  app = appname,
  consumer_key = key,
  consumer_secret = secret,
  access_token = access_token,
  access_secret = access_secret)


test_token <- get_token()
# uncomment below to test that the token is stable:
# test_token

```

##### Step 2: Collect tweets by hashtag
Gather tweets for #covid as a preliminary test:

```{r get_raw_data, echo=F}

get.tweets <- readline(prompt = "Would you like to search today's tweets? (y/n): ")

if (get.tweets == "y") {
  query <- readline(prompt = "Please type the hashtag you want to search (exclude the #): ")
  num_tweets <- as.numeric(readline(prompt = "How many tweets? (limit 18000): "))
  hashtag <- paste0("#", query)
  raw_tweets <- search_tweets(q = hashtag, 
                              n = num_tweets,
                              include_rts = FALSE,
                              `-filter` = "replies",
                              lang = "en")
  names(raw_tweets)

} else if(get.tweets == "n") {
  readline(prompt = "ok then. Loading covid19_tweets.csv back in ... ")
  # Load data back in from large CSV file
  tweets_to_mine <- fread("covid19_tweets.csv", fill = TRUE, sep="|")
  readline("Done! Have fun text-mining :)")
} else {
  print("sorry, didn't catch that.")
}

rm(get.tweets)

```


##### Step 3: Select vars you want to mine

```{r choose_vars, echo=F}
# list vars you want (change as needed)
vars_to_mine <- c("created_at",
                  "screen_name",
                  "text", 
                  "favorite_count", 
                  "retweet_count")

tweets_to_mine <- select(raw_tweets, all_of(vars_to_mine))

# check file size before saving
format(object.size(tweets_to_mine), units = "auto")

```



```{r save_data, echo=F}

# create file name
query_csv <- paste0(query, "_tweets.csv")

## save to CSV using fwrite (it's faster)
fwrite(tweets_to_mine, file=query_csv, sep="|")



```

##### Step 3: Follow text-mining protocol

To start, work through this tutorial from Jan 2020: https://medium.com/@traffordDataLab/exploring-tweets-in-r-54f6011a193d

```{r explore_data, echo=F}

# graph tweets by frequency over time
ts_plot(tweets_to_mine, by = "hours") +
  geom_line(color = "red") +
  labs(x = NULL, y = NULL,
       title = "Frequency of tweets with a #covid19 hashtag",
       subtitle = paste0(format.Date(min(tweets_to_mine$created_at), "%d %B %Y"), " to ", format.Date(max(tweets_to_mine$created_at), "%d %B %Y")),
       caption = "Data collected from Twitter's REST API via rtweet") + theme_minimal()


# top tweets by location
top_tweets_by_location <- raw_tweets %>%
  filter(!is.na(place_full_name)) %>% # ignore blank values
  count(place_full_name, sort = TRUE) %>%
  top_n(5)


# get top shared link

# first, unlist the urls_expanded_url


# WHY CAN'T I  MAKE A FUCKING TIBBLE???
urls <- (tibble(
  unlist(raw_tweets$urls_expanded_url, use.names =F)
  )
)

# count(list_of_links, sort = TRUE)
# top_n(5)


```